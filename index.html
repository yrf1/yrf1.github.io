<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html>
<head>
	<div class="navbar">
		<a href="./index.html"><font size="+1">Home</font></a>
    <a href="./group.html"><font size="+1">Group</font></a>
		<a href="./publication.html"><font size="+1">Publications</font></a>
    <a href="./teach.html"><font size="+1">Teaching</font></a>
    <a href="./awards.html"><font size="+1">Awards</font></a>
		<a href="./misc.html"><font size="+1">Misc</font></a>
	</div>

    <title>Hanjie Chen</title>
    <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
    <link rel="stylesheet" type="text/css" href="style.css"/>
</head>
<body style="color: rgba(0, 0, 0, 0.938);margin:0;padding:0">
<div id="wrapper">
  <div id="content-wrap">
    <div id="content">
      <div id="main">
        <tr>
        <p style="padding-right:30px;"> <img id="headshot" "float-right" align="right" alt="HanjieChen" src="photo/hanjie.jpeg" width="205" height="195"/></p>

            <p><font size="+3"><strong>&nbsp;Hanjie Chen</strong></font></p>
            <p></p>
            <p style="line-height:90%"><font size="3"><a href="https://csweb.rice.edu/" style="color:#9B0145;">&nbsp;&nbsp;Department of Computer Science</a></font></p>
            <p style="line-height:90%"><font size="3"><a href="https://www.rice.edu/" style="color:#9B0145;">&nbsp;&nbsp;Rice University</a></font></p>
            <p style="line-height:90%"><font size="3"><strong>&nbsp;&nbsp;Address:</strong> Duncan Hall 2081, 6100 Main St MS 364, Houston, TX 77005</font></p>
            <p style="line-height:90%"><font size="3"><strong>&nbsp;&nbsp;Email:</strong> hanjie@rice.edu</font></p>
            <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
            &nbsp;&nbsp;&nbsp;&nbsp;<a href="https://scholar.google.com/citations?user=DyYOgLwAAAAJ"><i class="ai ai-google-scholar ai-2x" style="font-size:26px; color:#9B0145">&nbsp;</i></a>
	    <a href="https://www.semanticscholar.org/author/Hanjie-Chen/7315244"><i class="ai ai-semantic-scholar ai-2x" style="font-size:26px; color:#9B0145">&nbsp;</i></a>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
            <a href="https://github.com/HanjieChen"><i class="fa fa-github" style="font-size:26px; color:#9B0145">&nbsp;</i></a>
            <a href="https://twitter.com/hanjie_chen?lang=en"><i class="fa fa-twitter" style="font-size:26px; color:#9B0145">&nbsp;</i></a>
	    <a href="https://www.linkedin.com/in/hanjie-chen-07a963195/"><i class="fa fa-linkedin-square" style="font-size:26px; color:#9B0145">&nbsp;</i></a>
        <!-- <p></p> -->

        </tr>
		<hr style="color:rgb(218, 218, 218);">

        <h1><a name="biography">About Me</h1>
        <p>I am an Assistant Professor in the <a href="https://csweb.rice.edu/" style="color:#9B0145;">Computer Science Department @ Rice University</a>. I am also affiliated with the <a href="https://kenkennedy.rice.edu/" style="color:#9B0145;">Ken Kennedy Institute</a>. I am broadly interested in Natural Language Processing, Interpretable Machine Learning, and Trustworthy AI. My research focuses on explaining and evaluating the properties, mechanisms, and capabilities of neural language models, enabling their alignment, interaction, and collaboration with humans, and enhancing their impact on real-world applications such as medicine, healthcare, sports, and more. By developing explainable AI techniques, I aim to make intelligent systems controllable by system developers, accessible to general users, applicable to various domains, and beneficial to society.
	<p>Prior to joining Rice, I was a Postdoctoral Fellow in the <a href="https://www.clsp.jhu.edu/" style="color:#9B0145;">Center for Language and Speech Processing @ Johns Hopkins University</a> from 2023 to 2024, hosted by Dr. <a href="https://www.cs.jhu.edu/~mdredze/" style="color:#9B0145;">Mark Dredze</a>. I completed my Ph.D. in Computer Science in May 2023 at the <a href="https://www.virginia.edu/" style="color:#9B0145;">University of Virginia</a>, where my advisor was Dr. <a href="http://yangfengji.net/" target="_blank" style="color:#9B0145;">Yangfeng Ji</a>. 
<!--           <p style="font-size:16px">&#11088; I will join the <a href="https://csweb.rice.edu/" style="color:#01569b;">Department of Computer Science @ Rice University</a> as a tenure-track Assistant Professor starting July 2024. I'm actively looking for motivated students to join my group. Please feel free to reach out (<a style="color:#01569b;">hanjie@rice.edu</a>) if you are interested in collaborating or working with me.</p> -->
<!-- 	  <p style="font-size:16px">&#128293; <a style="color:#d93b3b;"><b>Prospective students</b></a>: I am recruiting 1-2 PhD students for Fall 2024. Please apply to our <a href="https://csweb.rice.edu/academics/graduate-programs/graduate-admission" style="color:#01569b;">graduate programs</a>.</p>    -->
<!--           <dl style="background-color:#b9a2100f">
            <p></p>
            <p style="font-size:18px">&#128204; <a style="color:#6c3a3af2;"><b>Research Overview</b></a></p>
              <p style="padding-left:0px;"> <img align="left" src="overview.png" width="300" style="border: 0;" />
                <ul style="overflow: hidden; padding-right:10px;">
                <li> <b>How to understand black-box models?</b>
                  <br><a><font size="2">We develop explanation methods to explain model
                     <b>predictions</b> (<a href="http://aclanthology.lst.uni-saarland.de/2020.acl-main.494.pdf" style="color:#b04605;">ACL'2020</a>, 
                     <a href="https://aclanthology.org/2021.naacl-main.306.pdf" style="color:#b04605;">NAACL'2021</a>) 
                     and <b>uncertainty</b> (<a href="https://arxiv.org/pdf/2201.03742.pdf" style="color:#b04605;">AAAI-UDM'2023</a>) 
                     and evaluate explanations to understand model <b>reasoning</b> (<a href="https://arxiv.org/pdf/2210.04982.pdf" style="color:#b04605;">ACL'2023</a>)
                     and <b>robustness</b> (<a href="https://arxiv.org/pdf/2212.05327.pdf" style="color:#b04605;">BlackboxNLP'2022</a>, 
                     <a href="https://arxiv.org/pdf/2108.04990.pdf" style="color:#b04605;">2021</a>). </font></a>
                <li> <b>How to build trustworthy models?</b>
                  <br><a><font size="2">To cultivate trustworthiness in <b>existing</b> models, we have designed 
                    <b>data augmentation</b> (<a href="https://arxiv.org/pdf/1909.04225.pdf" style="color:#b04605;">NeurIPS WS'2019</a>), 
                    <b>variational word masks</b> (<a href="https://aclanthology.org/2020.emnlp-main.347v2.pdf" style="color:#b04605;">EMNLP'2020</a>), 
                    and <b>interaction graphs</b> (<a style="color:#b04605;">AAAI'2023</a>), 
                    to make model decision-making transparent and reliable. </font></a>
                <li> <b>How to improve models via explanations?</b>
                  <br><a><font size="2">We diagnose and debug models, especially their <b>robustness</b> and <b>fairness</b>, 
                    through the lens of explanations and develop solutions to improve them (<a href="https://arxiv.org/pdf/2203.12709.pdf" style="color:#b04605;">AAAI'2022</a>,
                    <a href="https://arxiv.org/pdf/2204.08039.pdf" style="color:#b04605;">ACL WS'2022</a>).
                     </font></a>
              </ul>
              <a style="color:#6c3a3af2; font-size:18px"><b>&nbsp;&nbsp; Future Research Themes&#10024;</b></a>
              <ul>
                <li> <b>Trustworthy and Responsible NLP:</b> model interpretability/explainability, robustness, fairness, efficiency
                <li> <b>Human-AI Interaction and Collaboration:</b> model evaluation/analysis, diagnosis/debugging, interactive AI systems
                <li> <b>NLP for Social Good:</b> applications in medicine/healthcare, psychology, education, social equality and inclusion
              </ul>
          </dl> -->
        
<!--           <dl style="background-color:#b953100c"> -->
	    <dl style="background-color:#b9a2100f">
            <p></p>
            <p style="font-size:18px"> <a style="color:#6c3a3af2;"><b>Highlights</b></a></p>
            <p style="font-size:16px">&#127891; Received the Outstanding Doctoral Student Award, UVA, 2023</p>
            <p style="font-size:16px">&#127881; Received the John A. Stankovic Graduate Research Award, UVA, 2023</p>
            <p style="font-size:16px">&#127881; I was awarded the Carlos and Esther Farrar Fellowship, 2022 - 2023</p>
            <p style="font-size:16px">&#128105;&#8205;&#127979; Received the UVA CS Outstanding Graduate Teaching Award and University-wide Graduate Teaching Awards Nominee (<b>top 5%</b> of graduate instructors) for the course, <a href="https://uvanlp.org/iml-2022/" target="_blank" style="color:#9B0145;">CS 6501/4501 Interpretable Machine Learning</a>, I co-designed and instructed at UVA in Spring 2022 </p>
            <p style="font-size:16px">&#128221; I maintain a <a href="https://github.com/HanjieChen/Reading-List/wiki" style="color:#9B0145;">Reading List</a> with interesting papers</p>
            <p></p>
          </dl>
        
<!--           <dl style="background-color:rgba(230, 199, 243, 0.13)"> -->
	    <dl style="background-color:#b953100c">
            <p></p>
            <a style="color:#6c3a3af2; font-size:18px"><b>&nbsp;&nbsp;Updates</b></a>  
              <div class="myBox">
                <ul>
		  <li> [02/2025] Invited talk, <i>Explanation Generation and Evaluation for (Multimodal) Large Language Models</i>, <a href="https://tamids.tamu.edu/event/ethical-and-explainable-geoai-workshop/" style="color:#9B0145;"> Ethical and Explainable GeoAI Workshop @ Texas A&M University</a>
		  <li> [02/2025] Panel Discussion on <a href="https://news.rice.edu/news/2025/rice-computer-science-department-invites-houston-industry-community-partners-discussion" style="color:#9B0145;">Large Language Models, DeepSeek, and the Future of Generative AI @ Rice</a>
		  <li> [01/2025] One paper accepted to <a href="https://iclr.cc/Conferences/2025" style="color:#9B0145;">ICLR 2025</a>, three papers accepted to <a href="https://2025.naacl.org" style="color:#9B0145;">NAACL 2025</a>&nbsp;&#127881;
		  <li> [12/2024] Invited talk, <i>From Passion to Profession: My Journey as a Woman in STEM</i>, <a href="https://sites.google.com/houstonisd.org/bellaireclubs/girls-who-code" style="color:#9B0145;"> Girls Who Code Club @ Bellaire High School, Houston</a>
		  <li> [11/2024] Organize the <a href="https://blackboxnlp.github.io" style="color:#9B0145;">BlackboxNLP Workshop @ EMNLP 2024</a>
		  <li> [11/2024] Invited talk, <i>Incredible Yet Limited Large Language Models in the Wild</i>, <a href="https://www.cs.fsu.edu" style="color:#9B0145;"> CS @ FSU</a>
		  <li> [10/2024] New preprint, <i>Language Models are Symbolic Learners in Arithmetic</i>, <a href="https://arxiv.org/pdf/2410.15580" style="color:#9B0145;"> arXiv</a>
	          <li> [10/2024] New preprint, <i>MiCEval: Unveiling Multimodal Chain of Thought’s Quality via Image Description and Reasoning Steps</i>, <a href="https://arxiv.org/pdf/2410.14668" style="color:#9B0145;"> arXiv</a>
		  <li> [10/2024] New preprint, <i>From Babbling to Fluency: Evaluating the Evolution of Language Models in Terms of Human Language Acquisition</i>, <a href="https://arxiv.org/pdf/2410.13259" style="color:#9B0145;"> arXiv</a>
		  <li> [10/2024] New preprint, <i>SPORTU: A Comprehensive Sports Understanding Benchmark for Multimodal Large Language Models</i>, <a href="https://arxiv.org/pdf/2410.08474" style="color:#9B0145;"> arXiv</a>
		  <li> [07/2024] Invited to serve as a Senior Area Chair for <a href="https://2025.aclweb.org/" style="color:#9B0145;"> ACL 2025</a>
		  <li> [06/2024] Tutorial on <a href="https://explanation-llm.github.io" style="color:#9B0145;"> Explanation in the Era of Large Language Models @ NAACL 2024</a>
		  <li> [06/2024] Invited to serve as an Area Chair for <a href="https://2024.emnlp.org/" style="color:#9B0145;"> EMNLP 2024</a>
	    <li> [04/2024] New preprint, <i>Will the Real Linda Please Stand up...to Large Language Models? Examining the Representativeness Heuristic in LLMs</i>, <a href="https://arxiv.org/pdf/2404.01461.pdf" style="color:#9B0145;"> arXiv</a>
		  <li> [03/2024] Invited talk at <a href="https://www.dwih-newyork.org/en/event/owl-ai-dc/" style="color:#9B0145;"> How Sustainable is Artificial Intelligence?</a>
		  <li> [02/2024] New preprint, <i>Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions</i>, <a href="https://arxiv.org/pdf/2402.18060.pdf" style="color:#9B0145;"> arXiv</a>
		  <li> [02/2024] New preprint, <i>RORA: Robust Free-Text Rationale Evaluation</i>, <a href="https://arxiv.org/pdf/2402.18678.pdf" style="color:#9B0145;"> arXiv</a>
		  <li> [01/2024] Co-teach the course <a href="https://sites.google.com/view/jhu-trustnlp/" style="color:#9B0145;">Trustworthy and Responsible NLP</a> with <a href="https://sharonlevy.github.io" style="color:#9B0145;">Sharon Levy</a>, Spring 2024, JHU
		  <li> [12/2023] Co-host the 22nd <a href="https://mp.weixin.qq.com/s/MwDsxsWBuXamf1HjXaePHw" style="color:#9B0145;">MLNLP Seminar</a>
		  <li> [11/2023] Guest lecture on <i>Interpretable and Explainable NLP</i> at <a href="https://jhu-intro-hlt.github.io/" style="color:#9B0145;"> 601.467/667 Introduction to Human Language Technology @ JHU</a>
	          <li> [11/2023] Guest lecture on <i>Interpretable and Explainable NLP</i> at <a href="https://jyzhao.net/teaching/csci699_2023fall.html" style="color:#9B0145;"> CSCI 699 Ethics in NLP @ USC</a>
		  <li> [10/2023] I will co-organize the <i>BlackboxNLP 2024</i> Workshop at <a href="https://hanjiechen.github.io/" style="color:#9B0145;"> EMNLP 2024</a>
		  <li> [10/2023] Our tutorial <i>Explanation in the Era of Large Language Models</i> is accepted to appear at <a href="https://2024.naacl.org/" style="color:#9B0145;"> NAACL 2024</a>
		  <li> [09/2023] New preprint, <i>Explainability for Large Language Models: A Survey</i>, <a href="https://arxiv.org/pdf/2309.01029.pdf" style="color:#9B0145;"> arXiv</a>
                  <li> [05/2023] <i>REV: Information-Theoretic Evaluation of Free-Text Rationales</i> is accepted by <a href="https://2023.aclweb.org/" style="color:#9B0145;"> ACL 2023</a>
                  <li> [03/2023] Invited Talk on <i>Bridging the Trustworthy Gap between AI and Humans: Interpretation Techniques for Modern NLP</i> at the <a href="https://www.clsp.jhu.edu/events/hanjie-chen-university-of-virginia/?instance_id=3253#.ZFFSr-zMJz8" style="color:#9B0145;">CLSP Seminar @ Johns Hopkins University</a>
<!--                   <li> [12/2022] Presentation on <i>Information-Theoretic Evaluation of Free-Text Rationales with Conditional V-Information</i> at <a href="https://tsrml2022.github.io/" style="color:#9B0145;">Trustworthy and Socially Responsible Machine Learning (TSRML) Workshop @ NeurIPS</a>
                  <li> [11/2022] Presentation on <i>Explaining Predictive Uncertainty by Looking Back at Model Explanations</i> at <a href="https://wimlworkshop.org/" style="color:#9B0145;">WiML Workshop 2022 @ NeurIPS</a>
                  <li> [10/2022] Talk on <i>REV: Information-Theoretic Evaluation of Free-Text Rationales</i> <a href="https://allenai.org/" style="color:#9B0145;">@ Allen Institute for AI (AI2)</a>
                  <li> [05/2022] Paper presentation on <i>Pathologies of Pre-trained Language Models in Few-shot Fine-tuning</i> at <a href="https://insights-workshop.github.io/index" style="color:#9B0145;">Insights Workshop @ ACL 2022</a>
                  <li> [02/2022] Paper presentation on <i>Adversarial Training for Improving Model Robustness? Look at Both Prediction and Interpretation</i> <a href="https://aaai.org/Conferences/AAAI-22/" style="color:#9B0145;">@ AAAI 2022</a>
                  <li> [12/2021] Invited talk on <i>Improving Model Robustness via Interpretation-based Adversarial Training</i> <a href="https://mp.weixin.qq.com/s/pc79dVgaaoBcBrKwmObLEQ" style="color:#9B0145;">@ MLNLP</a>
                  <li> [12/2021] Presentation on <i>Adversarial Training for Improving Model Robustness? Look at Both Prediction and Interpretation</i> at <a href="https://wimlworkshop.org/" style="color:#9B0145;">WiML Workshop 2021 @ NeurIPS</a>
                  <li> [12/2021] Presentation on <i>Adversarial Training for Improving Model Robustness? Look at Both Prediction and Interpretation</i> <a href="https://engineering.virginia.edu/events/2021-fall-cs-research-symposium" style="color:#9B0145;">@ 2021 Fall UVA CS Research Symposium</a>
                  <li> [06/2021] Paper presentation on <i>Explaining Neural Network Predictions on Sentence Pairs via Learning Word-Group Masks</i> <a href="https://2021.naacl.org/" style="color:#9B0145;">@ NAACL 2021</a>
                  <li> [04/2021] 2021 CRA-WP Grad Cohort for Women Workshop
                  <li> [03/2021] Poster presentation <a href="https://capwic.org/#:~:text=CAPWIC%20is%20the%20ACM%20Capital,supporters%20of%20women%20in%20computing." style="color:#9B0145;">@ ACM Capital Region Celebration of Women in Computing (CAPWIC)</a>
                  <li> [11/2020] Paper presentation on <i>Learning Variational Word Masks to Improve the Interpretability of Neural Text Classifiers</i> <a href="https://2020.emnlp.org/" style="color:#9B0145;">@ EMNLP 2020</a>
                  <li> [08/2020] Presentation on <i>Learning Variational Masks for Explainable Next Utterance Prediction in Dialog Systems</i> at IBM Research
                  <li> [07/2020] Paper presentation on <i>Generating Hierarchical Explanations on Text Classification via Feature Interaction Detection</i> <a href="https://acl2020.org/" style="color:#9B0145;">@ ACL 2020</a>
                  <li> [12/2019] Poster presentation on <i>Improving the Explainability of Neural Sentiment Classifiers via Data Augmentation</i> <a href="https://sites.google.com/view/robust-ai-in-fs-2019/home" style="color:#9B0145;">@ NeurIPS 2019 Workshop on Robust AI in Financial Services</a>
                  <li> [10/2019] Poster presentation on <i>Building Hierarchical Interpretations in Natural Language via Feature Interaction Detection</i> <a href="https://engineering.virginia.edu/cs-research-symposium-fall-2019" style="color:#9B0145;">@ UVA CS Research Symposium Fall 2019</a>
                  <li> [04/2019] Invited talk on <i>How to Train a More Interpretable Neural Text Classifier</i>, <a href="https://uvaml.github.io/" style="color:#9B0145;">AIML-Seminar @ UVA</a> -->
<!--                   <li> [11/2018] Poster presentation on <i>An Empirical Comparison on Convolutional and Recurrent Neural Networks for NLP</i> at the JUMP Undergraduate Research Initiative, UVA -->
                </ul>
              </div>
          </dl>

        <p></p>
        <h1><a name="activities">Research Experience</h1>
          <!-- <b><font size="+1">&nbspService</b> -->
          <ul>
              <li> <b>Johns Hopkins University</b>, Baltimore, MD, June 2023 - June 2024
                <p><i>Center for Language and Speech Processing</i></p>
                <ul>
                <li> Postdoctoral Fellow
                <li> Advisor: <a href="https://www.cs.jhu.edu/~mdredze/" style="color:#9B0145;">Mark Dredze</a>
              </ul>
              <li> <b>University of Virginia</b>, Charlottesville, VA, Aug. 2018 - May 2023
                <p><i>Information and Language Processing Lab</i></p>
                <ul>
                <li> Research Assistant
                <li> Advisor: <a href="http://yangfengji.net/" target="_blank" style="color:#9B0145;">Yangfeng Ji</a>
                </ul>
              <li> <b>Allen Institute for AI (AI2)</b>, Seattle, WA, May 2022 - Oct. 2022
                <p><i>Mosaic Group</i></p>
                <ul>
                <li> Research Intern
                <li> Manager: <a href="https://homes.cs.washington.edu/~yejin/" target="_blank" style="color:#9B0145;">Yejin Choi</a>
                <li> Mentors: <a href="https://swabhs.com/" target="_blank" style="color:#9B0145;">Swabha Swayamdipta</a>, <a href="https://users.soe.ucsc.edu/~hannahbrahman/" target="_blank" style="color:#9B0145;">Faeze Brahman</a>, <a href="https://shanzhenren.github.io/" target="_blank" style="color:#9B0145;">Xiang Ren</a>
                </ul>
              <li> <b>Microsoft Research</b>, Redmond, WA, May 2021 - Aug. 2021
                <p><i>Language and Information Technologies Group</i></p>
                <ul>
                <li> Research Intern
                <li> Manager: <a href="https://www.microsoft.com/en-us/research/people/hassanam/" target="_blank" style="color:#9B0145;">Ahmed H. Awadallah</a>
                <li> Mentors: <a href="https://www.microsoft.com/en-us/research/people/zheng/" target="_blank" style="color:#9B0145;">Guoqing Zheng</a>, <a href="https://www.linkedin.com/in/srinagesh-sharma-313536156/" target="_blank" style="color:#9B0145;">Srinagesh Sharma</a>
                </ul>
              <li> <b>IBM Research</b>, New York, NY, Jun. 2020 - Aug. 2020
                <p><i>Thomas J. Watson Research Center</i></p>
                <ul>
                <li> Research Intern
                <li> Manager: <a href="https://www.linkedin.com/in/luis-lastras-9a862a14/" target="_blank" style="color:#9B0145;">Luis Lastras</a>
                <li> Mentors: <a href="https://researcher.watson.ibm.com/researcher/view.php?person=ibm-chulaka.gunasekara" target="_blank" style="color:#9B0145;">Chulaka Gunasekara</a>, <a href="https://songfeng.github.io/" target="_blank" style="color:#9B0145;">Song Feng</a>, <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-hwan" target="_blank" style="color:#9B0145;">Hui Wan</a>, <a href="https://jatinganhotra.com/" target="_blank" style="color:#9B0145;">Jatin Ganhotra</a>, <a href="https://researcher.watson.ibm.com/researcher/view.php?person=in-jsachind" target="_blank" style="color:#9B0145;">Sachindra Joshi</a>
                </ul>

          </ul>
        <h1><a name="activities">Professional Service</h1>
          <!-- <b><font size="+1">&nbspService</b> -->
          <ul>
	      <li> <b>Organizer</b>: <a href="https://blackboxnlp.github.io" style="color:#9B0145;">BlackboxNLP Workshop @ EMNLP 2024 - 2025</a>
              <li> <b>Senior Area Chair</b>: ACL 2025
              <li> <b>Area Chair</b>: NAACL 2025, COLING 2025, EMNLP 2024, ACL ARR 2024 - Now, WiML Workshop @ NeurIPS 2022
              <li> <b>Program Committee</b>: ACL 2023, AAAI 2023, EMNLP 2021 - 2023, NAACL 2021, EACL 2023, CoNLL 2021 - 2022, NLPCC 2022, ACL DialDoc Workshop 2022, EMNLP BlackboxNLP Workshop 2021, 2023, NeurIPS Explainable AI Approaches for Debugging and Diagnosis Workshop 2021, Document-grounded Dialogue Workshop 2021, MASC-SLL 2020
              <li> <b>Reviewer</b>: TACL 2023 - 2025, ICLR 2024 - 2025, COLM 2024 - 2025, NeurIPS 2023 - 2025, EMNLP 2023, ACL ARR 2021 - 2024, ACL 2020 - 2021, EMNLP BlackboxNLP Workshop 2022, CoNLL 2019 - 2020, NLPCC 2019 - 2021
	      <li> <b>Mentor</b> for the Summer Undergraduate Research Fellowship (SURF) Program @ Rice, 2025
	      <li> <b>Diversity Representative</b> for UVA Computer Science Graduate Student Group (CSGSG) Council, 2022
          </ul>
	  <h1><a name="activities">Tutorial</h1>
          <ul>
	      <li> <a href="https://explanation-llm.github.io" style="color:#9B0145;"> Explanation in the Era of Large Language Models @ NAACL 2024</a>
          </ul>

		<hr style="color:rgb(218, 218, 218);">
		<div style="text-align: center; font-size: 18px;"><small>Last update: 02/2025</small>
		</div>
		<br>
      </div>
    </div>
  </div>
</div>

</body>
</html>
